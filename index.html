<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>MoonJeong Park</title> <meta name="author" content="MoonJeong Park"/> <meta name="description" content="MoonJeong's personal webpage. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="moonjeong"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/vs.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üó∫Ô∏è</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://jeong27.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/zenburn.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6D%6A%65%6F%6E%67%70@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=EY_FX_sAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Jeong27" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/moonjeong-park-97ba85258" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/CV_MoonJeong.pdf" target="_blank" rel="noopener noreferrer">Curriculum Vitae</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">MoonJeong</span> Park </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/profile_cand_icml_v3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/profile_cand_icml_v3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/profile_cand_icml_v3-1400.webp"></source> <img src="/assets/img/profile_cand_icml_v3.jpeg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="profile_cand_icml_v3.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>I am a Ph.D. student at the <a href="https://ml.postech.ac.kr/" target="_blank" rel="noopener noreferrer">POSTECH Machine Learning Lab</a>, under the guidance of Prof. <a href="https://dongwookim-ml.github.io/" target="_blank" rel="noopener noreferrer">Dongwoo Kim</a>, within the <a href="https://ai.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Graduate School of Artificial Intelligence</a> at <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">POSTECH</a>.</p> <p>My research interest lies in various aspects of machine learning, but not limited to, Graph Neural Network and Dynamical systems. I recently interested in interpreting deep learning network architectures and training methods through the lens of Ordinary Differential Equations (ODEs), and develop innovative and efficient neural network designs by integrating advanced mathematical principles with practical applications.</p> <p>I‚Äôm actively seeking opportunities for meaningful collaborations and internships. If you find my work interesting or have any questions, please don‚Äôt hesitate to reach out.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width:16%">Jun 5, 2025</th> <td> üìö A <a href="https://arxiv.org/abs/2506.04653" target="_blank" rel="noopener noreferrer">paper</a> challenging the widespread belief about the influence of oversmoothing in deep GNNs is now available on arXiv. </td> </tr> <tr> <th scope="row" style="width:16%">Jun 5, 2025</th> <td> üìö A <a href="https://arxiv.org/abs/2506.04694" target="_blank" rel="noopener noreferrer">paper</a> suggests influence fuction for GNNs is now available on arXiv. </td> </tr> <tr> <th scope="row" style="width:16%">Mar 3, 2025</th> <td> üìö A <a href="https://arxiv.org/abs/2503.01658" target="_blank" rel="noopener noreferrer">paper</a> about Personalizing LLMs with graph-based collaborative filtering framework is uploaded to arXiv preprint. </td> </tr> <tr> <th scope="row" style="width:16%">Oct 7, 2024</th> <td> üìö A <a href="https://arxiv.org/abs/2410.04824" target="_blank" rel="noopener noreferrer">paper</a> about analysis of gradient in GNN is uploaded to arXiv preprint. </td> </tr> <tr> <th scope="row" style="width:16%">Jun 1, 2024</th> <td> üìö A <a href="https://arxiv.org/abs/2406.00410" target="_blank" rel="noopener noreferrer">paper</a> about label smoothing in GNN is uploaded to arXiv preprint. </td> </tr> </table> </div> </div> <div class="education"> <h2>Education</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Sep, 2022 - Present</th> <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br> Ph.D. student in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> Advisor: <a href="https://dongwookim-ml.github.io/" target="_blank" rel="noopener noreferrer">Dongwoo Kim</a> </td> </tr> <tr> <th scope="row">Sep, 2019 - Sep, 2022</th> <td> <a href="https://postech.ac.kr/eng/" target="_blank" rel="noopener noreferrer">Pohang University of Science and Technology (POSTECH)</a>, Pohang, South Korea <br> Integrated M.S. student in <a href="https://cse.postech.ac.kr/" target="_blank" rel="noopener noreferrer">Computer Science and Engineering</a> <br> Advisor: <a href="https://dongwookim-ml.github.io/" target="_blank" rel="noopener noreferrer">Dongwoo Kim</a> </td> </tr> <tr> <th scope="row">Mar, 2014 - Sep, 2019</th> <td> <a href="https://dgist.ac.kr/en/" target="_blank" rel="noopener noreferrer">Daegu Gyeongbuk Institute of Science and Technology (DGIST)</a>, Daegu, South Korea <br> B.S. in School of Undergraduate Studies </td> </tr> </table> </div> </div> <div class="experience"> <h2>Experience</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row">August, 2020 - June, 2022</th> <td> <a href="https://piai.postech.ac.kr/academy-for-enterprise" target="_blank" rel="noopener noreferrer">POSCO AI education program</a>, Pohang, South Korea <br> <em>Teaching Assistant</em> <ul> <li>Participate as a teaching assistant for machine learning and deep learning.</li> </ul> </td> </tr> <tr> <th scope="row">June, 2018 - January, 2019</th> <td> <a href="https://datalab.snu.ac.kr/en" target="_blank" rel="noopener noreferrer">Data Mining Lab</a> at <a href="https://www.snu.ac.kr/" target="_blank" rel="noopener noreferrer">Seoul National University</a>, Seoul, South Korea <br> <em>Research Intern</em> <ul> <li>Mentor: <a href="https://datalab.snu.ac.kr/~ukang/" target="_blank" rel="noopener noreferrer">Prof. U Kang</a> and <a href="https://leesael.github.io/" target="_blank" rel="noopener noreferrer">Prof. Sael Lee</a> </li> <li>Participate in research projects about sparse tucker factorization for large-scale tensor</li> </ul> </td> </tr> <tr> <th scope="row">June, 2016 - July, 2016</th> <td> <a href="https://msrl.ethz.ch/" target="_blank" rel="noopener noreferrer">Multi-Scale Robotics Lab</a> at <a href="https://ethz.ch/en.html" target="_blank" rel="noopener noreferrer">Eidgen√∂ssische Technische Hochschule Z√ºrich</a>, Z√ºrich, Switzerland <br> <em>Research Intern</em> <ul> <li>Mentor: Dr. Carmela De Marco</li> <li>Participate in research projects about fabrication of microrobot to acquire single cell</li> </ul> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Publications</h2> <p>* indicates equal contribution.</p> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2025fallacy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2025fallacy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2025fallacy-1400.webp"></source> <img src="/assets/img/park2025fallacy.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2025fallacy" class="col-sm-8"> <div class="title">The Oversmoothing Fallacy: A Misguided Narrative in GNN Research</div> <div class="author"> MoonJeong Park,¬†Sunghyun Choi,¬†Jaeseung Heo,¬†Eunhyeok Park,¬† and Dongwoo Kim </div> <div class="periodical"> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2506.04653" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Oversmoothing has been recognized as a main obstacle to building deep Graph Neural Networks (GNNs), limiting the performance. This position paper argues that the influence of oversmoothing has been overstated and advocates for a further exploration of deep GNN architectures. Given the three core operations of GNNs, aggregation, linear transformation, and non-linear activation, we show that prior studies have mistakenly confused oversmoothing with the vanishing gradient, caused by transformation and activation rather than aggregation. Our finding challenges prior beliefs about oversmoothing being unique to GNNs. Furthermore, we demonstrate that classical solutions such as skip connections and normalization enable the successful stacking of deep GNN layers without performance degradation. Our results clarify misconceptions about oversmoothing and shed new light on the potential of deep GNNs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/heo2025influence-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/heo2025influence-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/heo2025influence-1400.webp"></source> <img src="/assets/img/heo2025influence.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="heo2025influence" class="col-sm-8"> <div class="title">Influence Functions for Edge Edits in Non-Convex Graph Neural Networks</div> <div class="author"> Jaeseung Heo,¬†Kyeongheung Yun,¬†Seokwon Yoon,¬†MoonJeong Park,¬†Jungseul Ok,¬† and Dongwoo Kim </div> <div class="periodical"> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2506.04694" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Understanding how individual edges influence the behavior of graph neural networks (GNNs) is essential for improving their interpretability and robustness. Graph influence functions have emerged as promising tools to efficiently estimate the effects of edge deletions without retraining. However, existing influence prediction methods rely on strict convexity assumptions, exclusively consider the influence of edge deletions while disregarding edge insertions, and fail to capture changes in message propagation caused by these modifications. In this work, we propose a proximal Bregman response function specifically tailored for GNNs, relaxing the convexity requirement and enabling accurate influence prediction for standard neural network architectures. Furthermore, our method explicitly accounts for message propagation effects and extends influence prediction to both edge deletions and insertions in a principled way. Experiments with real-world datasets demonstrate accurate influence predictions for different characteristics of GNNs. We further demonstrate that the influence function is versatile in applications such as graph rewiring and adversarial attacks.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/choi2025copl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/choi2025copl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/choi2025copl-1400.webp"></source> <img src="/assets/img/choi2025copl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="choi2025copl" class="col-sm-8"> <div class="title">CoPL: Collaborative Preference Learning for Personalizing LLMs</div> <div class="author"> Youngbin Choi,¬†Seunghyuk Cho,¬†Minjong Lee,¬†MoonJeong Park,¬†Yesong Ko,¬†Jungseul Ok,¬† and Dongwoo Kim </div> <div class="periodical"> 2025 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2503.01658" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Personalizing large language models (LLMs) is important for aligning outputs with diverse user preferences, yet existing methods struggle with flexibility and generalization. We propose CoPL (Collaborative Preference Learning), a graph-based collaborative filtering framework that models user-response relationships to enhance preference estimation, particularly in sparse annotation settings. By integrating a mixture of LoRA experts, CoPL efficiently fine-tunes LLMs while dynamically balancing shared and user-specific preferences. Additionally, an optimization-free adaptation strategy enables generalization to unseen users without fine-tuning. Experiments on UltraFeedback-P demonstrate that CoPL outperforms existing personalized reward models, effectively capturing both common and controversial preferences, making it a scalable solution for personalized LLM alignment.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2024taming-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2024taming-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2024taming-1400.webp"></source> <img src="/assets/img/park2024taming.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2024taming" class="col-sm-8"> <div class="title">Taming Gradient Oversmoothing and Expansion in Graph Neural Networks</div> <div class="author"> MoonJeong Park,¬† and Dongwoo Kim </div> <div class="periodical"> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2410.04824" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Oversmoothing has been claimed as a primary bottleneck for multi-layered graph neural networks (GNNs). Multiple analyses have examined how and why oversmoothing occurs. However, none of the prior work addressed how optimization is performed under the oversmoothing regime. In this work, we show the presence of \textitgradient oversmoothing preventing optimization during training. We further analyze that GNNs with residual connections, a well-known solution to help gradient flow in deep architecture, introduce \textitgradient expansion, a phenomenon of the gradient explosion in diverse directions. Therefore, adding residual connections cannot be a solution for making a GNN deep. Our analysis reveals that constraining the Lipschitz bound of each layer can neutralize the gradient expansion. To this end, we provide a simple yet effective normalization method to prevent the gradient expansion. An empirical study shows that the residual GNNs with hundreds of layers can be efficiently trained with the proposed normalization without compromising performance. Additional studies show that the empirical observations corroborate our theoretical analysis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/heo2024posterior-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/heo2024posterior-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/heo2024posterior-1400.webp"></source> <img src="/assets/img/heo2024posterior.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="heo2024posterior" class="col-sm-8"> <div class="title">Posterior Label Smoothing for Node Classification</div> <div class="author"> Jaeseung Heo,¬†MoonJeong Park,¬† and Dongwoo Kim </div> <div class="periodical"> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2406.00410" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>Soft labels can improve the generalization of a neural network classifier in many domains, such as image classification. Despite its success, the current literature has overlooked the efficiency of label smoothing in node classification with graph-structured data. In this work, we propose a simple yet effective label smoothing for the transductive node classification task. We design the soft label to encapsulate the local context of the target node through the neighborhood label distribution. We apply the smoothing method for seven baseline models to show its effectiveness. The label smoothing methods improve the classification accuracy in 10 node classification datasets in most cases. In the following analysis, we find that incorporating global label statistics in posterior computation is the key to the success of label smoothing. Further investigation reveals that the soft labels mitigate overfitting during training, leading to better generalization performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2024mitigating-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2024mitigating-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2024mitigating-1400.webp"></source> <img src="/assets/img/park2024mitigating.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2024mitigating" class="col-sm-8"> <div class="title">Mitigating Oversmoothing Through Reverse Process of GNNs for Heterophilic Graphs</div> <div class="author"> MoonJeong Park,¬†Jaeseung Heo,¬† and Dongwoo Kim </div> <div class="periodical"> <em>International Conference on Machine Learning (<b>ICML</b>),</em> 2024 </div> <span style="color:red"><b>Excellent Paper Award at BK21 Paper Award</b></span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2403.10543" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://youtu.be/JKN68PDYSSA?si=usk-U8KvA5oX2Mh2" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Video</a> <a href="https://github.com/ml-postech/reverse-gnn" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Graph Neural Network (GNN) resembles the diffusion process, leading to the over-smoothing of learned representations when stacking many layers. Hence, the reverse process of message passing can produce the distinguishable node representations by inverting the forward message propagation. The distinguishable representations can help us to better classify neighboring nodes with different labels, such as in heterophilic graphs. In this work, we apply the design principle of the reverse process to the three variants of the GNNs. Through the experiments on heterophilic graph data, where adjacent nodes need to have different representations for successful classification, we show that the reverse process significantly improves the prediction performance in many cases. Additional analysis reveals that the reverse mechanism can mitigate the over-smoothing over hundreds of layers. Our code is available at https://github.com/ml-postech/reverse-gnn.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2023spreme-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2023spreme-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2023spreme-1400.webp"></source> <img src="/assets/img/park2023spreme.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2023spreme" class="col-sm-8"> <div class="title">SpReME: Sparse Regression for Multi-Environment Dynamic Systems,</div> <div class="author"> MoonJeong Park*,¬†Youngbin Choi*,¬† and Dongwoo Kim </div> <div class="periodical"> <em>AAAI Conference on Artificial Intelligence, Workshop on When Machine Learning meets Dynamical Systems: Theory and Applications (<b>AAAIw</b>),</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2302.05942" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/ml-postech/SpReME" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This paper introduces a new weakly supervised learning approach for instance segmentation using extreme points, i.e., the topmost, leftmost, bottommost, and rightmost points of an object. Although these points are readily available in the modern bounding box annotation process and offer strong clues for precise segmentation, they have received less attention in the literature. Motivated by this, our study explores extreme point supervised instance segmentation to further enhance performance at the same annotation cost with box-supervised methods. Our work considers extreme points as a part of the true instance mask and propagates them to identify potential foreground and background points, which are all together used for training a pseudo label generator. Then pseudo labels given by the generator are in turn used for supervised learning of our final model. Our model generates high-quality masks, particularly when the target object is separated into multiple parts, where previous box-supervised methods often fail. On three public benchmarks, our method significantly outperforms existing box-supervised methods, further narrowing the gap with its fully supervised counterpart.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2022meta-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2022meta-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2022meta-1400.webp"></source> <img src="/assets/img/park2022meta.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2022meta" class="col-sm-8"> <div class="title">MetaSSD: Meta-Learned Self-Supervised Detection,</div> <div class="author"> MoonJeong Park,¬†Jungseul Ok,¬†Yo-Seb Jeon,¬† and Dongwoo Kim </div> <div class="periodical"> <em>IEEE International Symposium on Information Theory (<b>ISIT</b>),</em> 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/2205.15271" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/ml-postech/MetaSSD" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Deep learning-based symbol detector gains increasing attention due to the simple algorithm design than the traditional model-based algorithms such as Viterbi and BCJR. The supervised learning framework is often employed to train a model, where true symbols are necessary. There are two major limitations in the supervised approaches: a) a model needs to be retrained from scratch when new train symbols come to adapt to a new channel status, and b) the length of the training symbols needs to be longer than a certain threshold to make the model generalize well on unseen symbols. To overcome these challenges, we propose a meta-learning-based self-supervised symbol detector named MetaSSD. Our contribution is two-fold: a) meta-learning helps the model adapt to a new channel environment based on experience with various meta-training environments, and b) self-supervised learning helps the model to use relatively less supervision than the previously suggested learning-based detectors. In experiments, MetaSSD outperforms OFDM-MMSE with noisy channel information and shows comparable results with BCJR. Further ablation studies show the necessity of each component in our framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2022vest2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2022vest2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2022vest2-1400.webp"></source> <img src="/assets/img/park2022vest2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2022vest2" class="col-sm-8"> <div class="title">Large-scale tucker Tensor factorization for sparse and accurate decomposition,</div> <div class="author"> Jun-Gi Jang*,¬†MoonJeong Park*,¬†Jongwuk Lee,¬† and Lee Sael </div> <div class="periodical"> <em>The Journal of Supercomputing,</em> 2022 </div> <span style="color:red"><b>Extended version of the conference paper "VeST: Very Sparse Tucker Factorization of Large-Scale Tensors"</b></span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="https://link.springer.com/article/10.1007/s11227-022-04559-4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> </div> <div class="abstract hidden"> <p>How can we generate sparse tensor decomposition results for better interpretability? Typical tensor decomposition results are dense. Dense results require additional postprocessing for data interpretation, especially when the data are large. Thus, we present a large-scale Tucker factorization method for sparse and accurate tensor decomposition, which we call the Very Sparse Tucker factorization (VeST) method. The proposed VeST outputs highly sparse decomposition results from a large-scale partially observable tensor data. The approach starts by decomposing the input tensor data, then iteratively determining unimportant elements, removing them, and updating the remaining elements until a terminal state is reached. We define ‚Äòresponsibility‚Äô of each element on the reconstruction error to determine unimportant elements in the decomposition results. The decomposition results are updated iteratively in parallel using carefully constructed coordinate descent rules for scalable computation. Furthermore, the suggested method automatically looks for the optimal sparsity ratio, resulting in a balanced sparsity-accuracy trade-off. Extensive experiments using real-world datasets showed that our method produces more accurate results than that of the competitors. Experiments further showed that the proposed method is scalable in terms of the input dimensionality, the number of observable entries, and the thread count.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/park2021vest-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/park2021vest-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/park2021vest-1400.webp"></source> <img src="/assets/img/park2021vest.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="park2021vest" class="col-sm-8"> <div class="title">VeST: Very Sparse Tucker Factorization of Large-Scale Tensors,</div> <div class="author"> MoonJeong Park*,¬†Jun-Gi Jang*,¬† and Lee Sael </div> <div class="periodical"> <em>IEEE International Conference on Big Data and Smart Computing (<b>BigComp</b>),</em> 2021 </div> <span style="color:red"><b>Best Paper Award, 1st Place</b></span> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a> <a href="http://arxiv.org/abs/1904.02603" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Paper</a> <a href="https://github.com/leesael/vest" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Given a large tensor, how can we decompose it to sparse core tensor and factor matrices without reducing the accuracy? Existing approaches either output dense results or have scalability issues. In this paper, we propose VEST, a tensor factorization method for large partially observable data to output a very sparse core tensor and factor matrices. VEST performs initial decomposition and iteratively determines unimportant entries in the decomposition results, removes the unimportant entries, and updates the remaining entries. To determine unimportant entries of factor matrices and core tensor, we define and use the entry-wise ‚Äòresponsibility‚Äô of the current decomposition. For scalable computation, the entries are updated iteratively using a carefully derived coordinate descent rule in parallel. Also, VEST automatically searches for the best sparsity ratio that results in a balanced trade-off between sparsity and accuracy. Extensive experiments show that our method VEST produces more accurate results compared to the best performing competitors for all tested real-life datasets.</p> </div> </div> </div> </li> </ol> </div> <div class="honors"> <h2>Honors and Awards</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <td> <strong>BK21 Best Paper Award, POSTECH AIGS (2024)</strong> <ul> <li>Excellence Award - <em>Mitigating oversmoothing through reverse process of gnns for heterophilic graphs</em> (ICML2024)</li> </ul> </td> </tr> <tr> <td> <strong>Best Paper Award, IEEE BigComp (2021)</strong> <ul> <li>Best Paper Award, 1st Place - <em>VeST: Very Sparse Tucker Factorization of Large-Scale Tensors</em> (BigComp2021)</li> </ul> </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%6A%65%6F%6E%67%70@%70%6F%73%74%65%63%68.%61%63.%6B%72" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=EY_FX_sAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/Jeong27" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/moonjeong-park-97ba85258" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 MoonJeong Park. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>